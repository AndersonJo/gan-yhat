{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improved GAN\n",
    "\n",
    "해당 Notebook은 [Improved Techniques for Training GANs](https://arxiv.org/pdf/1606.03498.pdf) 을 기반으로 하였습니다.\n",
    "\n",
    "*https://github.com/AaronYALai/Generative_Adversarial_Networks_PyTorch/tree/master/ImprovedGAN\n",
    "*https://theneuralperspective.com/2016/11/12/improved-techniques-for-training-gans/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Matching\n",
    "\n",
    "## Summary\n",
    "\n",
    "* GAN학습이 Unstable할때 사용한다.\n",
    "* G를 학습할때 사용하는 새로운 objective function 이다.\n",
    "* D의 최종 classifier의 output을 사용하는 것이 아니라, D의 중간층에 있는 activations을 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition\n",
    "\n",
    "Feature Matching은 Generator에 대한 새로운 objective function이며 Discriminator가 over-training되는 것을 막음으로서 GAN의 instability를 다소 완화해줍니다. <br>\n",
    "일반적인 vanilla GAN의 경우 G의 objective는 minimax에서 $ \\log(1-D(G(z)) $ 를 최소화시키는 것입니다. <br>\n",
    "하지만 학습이 안되는 이유로 Heuristic method로 G의 objective를 $ \\log D(G(z)) $ 로 바꾸었으면 결론적으로 G는 해당 objective를 1로 maximize시키는 것입니다.\n",
    "\n",
    "Improved GAN의 논문에서 제안하는 새로운 G의 objective는 다음과 같습니다.<br>\n",
    "Discriminator의 output을 1로 maximize시키는 것이 아니라, G는 실제 데이터의 statistics와 일치가 되도록 만드는 것입니다.<br>\n",
    "구체적인 방법으로 Discriminator의 중간층에 있는 activation을 (activations on an intermediate layer of the discriminator) 이용하여 실제 데이터와 동일한 기대값을 갖은 statistics로 매칭을 시켜주게 됩니다.\n",
    "\n",
    "간단하게 말해서 Discriminator의 중간 지점의 하나의 레이어에 있는 activation의 outout값을 이용한다는 뜻입니다.<br>\n",
    "이때 실제데이터를 이용했을때에 나오는 D의 activation의 output과, G가 생성한 가짜 데이터를 feed forward 시켰을때 나오는 output의 차이를 갖고서 학습을 시켜준다는 뜻입니다. 공식보면 전혀 어려운 내용이 아닙니다.\n",
    "\n",
    "><span style=\"color:#777777;\">\n",
    "    Norm function은 다음과 같습니다.<br>\n",
    "    $ \\| x \\|_p = \\sqrt[p]{ \\sum_i | x_i |^p} $ <br><br>\n",
    "    Improved GAN의 논문에서 말하는 Feature Matching의 norm함수는 l2-norm을 말하는 것입니다.<br>\n",
    "    $ \\| x \\|_2 = \\sqrt{\\sum_i x^2_i} $ <br><br>\n",
    "    만약 l2 norm이 vector의 dirrerence에 사용되면 다음과 같습니다.<br>\n",
    "    $ \\|x_1 - x_2 \\|^2_2 =   \\sqrt{ \\sum_i ( x_1 - x_2 )^2}  $\n",
    "    \n",
    "></span>\n",
    "\n",
    "$$ \\begin{align}\n",
    "L_{G} &=  \\| \\mathbb{E}_{\\mathbf{x} \\sim P_{\\text{data}}} \\mathbf{f}(\\mathbf{x}) - \\mathbb{E}_{z \\sim P_z (z)} \\mathbf{f}(G(\\mathbf{z})) \\|^2_2  &[1] \\\\\n",
    "&= \\sum^N_i \\big[ \\mathbf{f}(x_i) -  \\mathbf{f}(G(z_i))  \\big]^2 &[2]\n",
    "\\end{align} $$\n",
    "\n",
    "\n",
    "* $ \\mathbf{f} $ 는 Discriminator의 중간층 하나의 레이어에 있는 특정 activation을 가르킵니다.\n",
    "* squared root의 경우 가장 밖깥쪽 제곱에 의해서 없어지게 됩니다.\n",
    "\n",
    "현실은 꼭 반드시 저런 공식을 쓸 필요는 없습니다. <br>\n",
    "그냥 MSE를 $ \\frac{1}{N} \\sum^N_i \\big[ \\mathbf{f}(x_i) -  \\mathbf{f}(G(z_i))  \\big]^2 $  쓰거나 Squre Root 를 씌워줘도 됩니다. <br>\n",
    "포인트는 loss값을 보면서 적당한 값이 나오도록 정하면 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* [IMPROVED TECHNIQUES FOR TRAINING GANS](https://theneuralperspective.com/2016/11/12/improved-techniques-for-training-gans/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
